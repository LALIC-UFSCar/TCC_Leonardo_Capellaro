{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZAHFNCj4St8"
   },
   "source": [
    "# Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvlavcVD4K7W",
    "outputId": "1dcef3d0-9006-40a4-cf10-0de07ab1da1f"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSB9XitjH9ZL",
    "outputId": "0674c52c-9789-431d-c028-42c56af44e63"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duF467PBNT2u"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install enelvo\n",
    "\n",
    "!pip install sentencepiece\n",
    "!pip install bertopic[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGVMHSE8z5PZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import nltk\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from nltk.corpus import stopwords\n",
    "from enelvo import normaliser\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "from transformers import *\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dxliMyUUuvS8",
    "outputId": "5d4a19f1-8b02-48c6-8bdc-c7f8c9a86d6c"
   },
   "outputs": [],
   "source": [
    "# Seleciona a CPU para processamento caso não haja GPU compatível com CUDA.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7X8kONn1PFM"
   },
   "outputs": [],
   "source": [
    "#Leitura da base de treinamento\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "#file = 'corpus_elec_para_bert_raw.csv'\n",
    "file = 'drive/MyDrive/bases_tcc/tweets_eleicao_enelvo_raw.csv'\n",
    "\n",
    "# converting json dataset from dictionary to dataframe\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df = df.drop(df[df.sentiment == 'brasil'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y25GiMTSotBv"
   },
   "outputs": [],
   "source": [
    "df.to_excel('df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdW-KmFG1jzT"
   },
   "outputs": [],
   "source": [
    "#Realiza o one-hot encoding na coluna 2º Filtro para o treinamento.\n",
    "one_hot = pd.get_dummies(df['sentiment'])\n",
    "df = df.drop('sentiment',axis = 1)\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZuTrNkA1lW4"
   },
   "outputs": [],
   "source": [
    "#Seleciona a linguagem do corretor para português e seleciona o dicionário personalizado.\n",
    "import re\n",
    "#spell.word_frequency.load_text_file('palavras.txt')\n",
    "\n",
    "def removePontuacao(sentenca): \n",
    "    sentenca = re.sub(u'[^a-zA-ZáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ: ]', r' ',sentenca)\n",
    "    sentenca = re.sub(r'[?|!|\\'|\"|#]',r'',sentenca)\n",
    "    sentenca = re.sub(r'[.|,|)|(|\\|/]',r' ',sentenca)\n",
    "    sentenca = sentenca.replace(\"\\n\",\" \")\n",
    "    return sentenca\n",
    "\n",
    "def removeruidos(sentenca):\n",
    "    #sentenca = sentenca.replace(\"username\",\"\")\n",
    "    sentenca = sentenca.replace(\"hashtag\",\"\")\n",
    "    sentenca = sentenca.replace(\"retweeet\",\"\")\n",
    "    return sentenca\n",
    "\n",
    "def corrige_internetes(sentenca):\n",
    "    norm = normaliser.Normaliser(sanitize = True, capitalize_inis=True, capitalize_pns=True, capitalize_acs=True)\n",
    "    norm_sentence = norm.normalise(sentenca)\n",
    "    return norm_sentence\n",
    "\n",
    "def deEmojify(sentenca):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',sentenca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfUzvfGb1qgt"
   },
   "outputs": [],
   "source": [
    "#Realiza o pré-processamento dos textos de comentários.\n",
    "#df[\"text\"] = df[\"text\"].swifter.allow_dask_on_strings(enable=True).apply(lambda x: x.lower())\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: removeruidos(x))\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: deEmojify(x))\n",
    "#df[\"text\"] = df[\"text\"].swifter.allow_dask_on_strings(enable=True).apply(lambda x: removePontuacao(x))\n",
    "#df[\"text\"] = df[\"text\"].swifter.allow_dask_on_strings(enable=True).apply(lambda x: corretorGramatical(x))\n",
    "#df[\"text\"] = df[\"text\"].swifter.allow_dask_on_strings(enable=True).apply(lambda x: corrige_internetes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "X59wlIQrfMXv",
    "outputId": "6d70389b-460b-4ab8-b0a0-80f297e8046a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHC6tZO419O1"
   },
   "outputs": [],
   "source": [
    "#Faz a importação do modelo BERT pré-treinado para pt-br.\n",
    "bert = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "\n",
    "#Carrega o BERT tokenizer para pt-br.\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSwG4Aqg2iMX"
   },
   "outputs": [],
   "source": [
    "#Obtém a lista de colunas que contém as classificações após o one hot encoding.\n",
    "cols = df.columns\n",
    "label_cols = list(cols[4:8])\n",
    "\n",
    "#Obtém o número de classificações possíveios.\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "#Embaralha as linhas do dataframe.\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Cria uma coluna contendo uma lista com os valores das colunas do one hot encoding.\n",
    "df['one_hot_labels'] = list(df[label_cols].values)\n",
    "\n",
    "#Obtém a lista todos os valores das labels e de todos os comentários\n",
    "labels = list(df.one_hot_labels.values)\n",
    "comments = list(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syllowtQ9Sj8"
   },
   "outputs": [],
   "source": [
    "label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWM2ufzjlrMl"
   },
   "outputs": [],
   "source": [
    "print('Count of 1 per label: \\n', df[label_cols].sum(), '\\n') # Label counts, may need to downsample or upsample\n",
    "print('Count of 0 per label: \\n', df[label_cols].eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbAnom7V2sVT",
    "outputId": "ce1cea23-71ad-437d-c584-60cc8d45e0ec"
   },
   "outputs": [],
   "source": [
    "#Atribui o valor máximo de tokens por comentário para 100 - Evita estouro de memória de GPU\n",
    "max_length = 140\n",
    "\n",
    "#Cria tokenizador BERT com os dados pré-treinados em português.\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "#Utiliza o método \"batch_encode_plus\" para obter os tokens, os tipos de tokens e as attention masks.\n",
    "encodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True)\n",
    "\n",
    "#Sentenças pré-treinadas tokenizadas e codificadas, token type ids e attention masks.\n",
    "input_ids = encodings['input_ids']\n",
    "token_type_ids = encodings['token_type_ids']\n",
    "attention_masks = encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYBAtcee2v9I"
   },
   "outputs": [],
   "source": [
    "#Identifica as entradas dos índices das one hot labels que ocorrem apenas uma vez. Isso permitirá realizarmos um split estratificado.\n",
    "label_counts = df.one_hot_labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "\n",
    "#Reune as entradas que possuem somente uma instância para forçar elas para os dados de treinamento após o split.\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W2iYAC1204v"
   },
   "outputs": [],
   "source": [
    "#Usa a função train_test_split para separar os dados em dados de treino e de validação. 10% para validação, 90% treino.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids, attention_masks,\n",
    "                                                            random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "#Adiciona os índices que apareceram uma única vez em dados de treino.\n",
    "train_inputs.extend(one_freq_input_ids)\n",
    "train_labels.extend(one_freq_labels)\n",
    "train_masks.extend(one_freq_attention_masks)\n",
    "train_token_types.extend(one_freq_token_types)\n",
    "\n",
    "#Converte todos os dados em torch tensors, o tipo de dado requerido pelo modelo.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_token_types = torch.tensor(validation_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVmW9rTP22wa"
   },
   "outputs": [],
   "source": [
    "#Atribui um tamanho de batch para o treinamento. Valores recomendados: 32, 48 ou 128. Valores altos podem estourar a memória da GPU.\n",
    "batch_size = 24\n",
    "\n",
    "#Cria um iterador para os dados com o torch DataLoader. Isso ajuda a economizamr memória durante o treinamento, pois ao contrário de um loop,\n",
    "#com um iterador não é necessário carregar o dataset inteiro para a memória.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hw7tHJuC26LA"
   },
   "outputs": [],
   "source": [
    "#Carrega o modelo pré-treinado. O modelo incluirá uma única camada linear de classificação no topo.\n",
    "model = BertForSequenceClassification.from_pretrained('neuralmind/bert-large-portuguese-cased', num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj-LzIyY3Bos"
   },
   "outputs": [],
   "source": [
    "#Configurando parâmetros de otimização customizados.\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IcjigdJ3DVY"
   },
   "outputs": [],
   "source": [
    "#Otimizador ADAMW\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
    "#optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CmXC2IjH3Fqb"
   },
   "outputs": [],
   "source": [
    "# Armazena a perda e a acurácia para plotagem.\n",
    "train_loss_set = []\n",
    "\n",
    "# Número de passagens para o treinamento.\n",
    "epochs = 12\n",
    "i=1\n",
    "# Trange permite exibir uma barra de progresso.\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "  # Treinamento\n",
    "\n",
    "  # Configura o modelo para o modo de treinamento.\n",
    "  model.train()\n",
    " \n",
    "  # Variáveis de rastreamento.\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "  # Treina os dados para uma passagem\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Adiciona o batch à GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Descompacta as entradas do dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    # Limpa os gradientes (por padrão eles acumulam)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Classificação multilabel\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    loss_func = BCEWithLogitsLoss() \n",
    "    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) # Converte as labels para float para o cálculo\n",
    "    train_loss_set.append(loss.item())   \n",
    "\n",
    "    # Passe para trás\n",
    "    loss.backward()\n",
    "    # Atualiza os parâmetros e dá um passo usando o gradiente calculado.\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Atualiza as variáveis de rastramento.\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\" Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "  # Validação\n",
    "\n",
    "  # Configura o modelo para o modo de avaliação para avaliar as perdas no conjunto de validação\n",
    "  model.eval()\n",
    "\n",
    "  # Variáveis para reunir os resultados completos.\n",
    "  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predição.\n",
    "  for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Descompacta as entradas do dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    with torch.no_grad():\n",
    "      # Passe para frente\n",
    "      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      b_logit_pred = outs[0]\n",
    "      pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "      pred_label = pred_label.to('cpu').numpy()\n",
    "      b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "  # \"Achata\" as saídas\n",
    "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "  true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "  # Calcula a acurácia\n",
    "  threshold = 0.50\n",
    "  pred_bools = [pl>threshold for pl in pred_labels]\n",
    "  true_bools = [tl==1 for tl in true_labels]\n",
    "  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "  \n",
    "  print(' Acurácia F1: ' + str(round(val_f1_accuracy,2)) + '%')\n",
    "  print('Acurácia Flat: ' + str(round(val_flat_accuracy,2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0Ju5e0lj907i"
   },
   "outputs": [],
   "source": [
    "# Salva o modelo.\n",
    "torch.save(model, '/content/drive/MyDrive/TCC/model_bert_tcc/bert_model_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "95ZLRVy9OnkS"
   },
   "outputs": [],
   "source": [
    "#model = torch.load('/content/drive/MyDrive/TCC/model_bert_tcc/bert_model_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9kfriJA2TSW"
   },
   "source": [
    "# Aplicação do modelo para Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WvmAwtPkcDDY"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "elzGTjgFcTSm"
   },
   "outputs": [],
   "source": [
    "df_classificacao = pd.read_csv('drive/MyDrive/bases_tcc/tweets_2turno.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jtXAIFEZR6H1",
    "outputId": "b9e43d2d-ac30-4b55-98a5-c5b716faf6e7"
   },
   "outputs": [],
   "source": [
    "df_classificacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ycq1z5dCdlFx"
   },
   "outputs": [],
   "source": [
    "test_df = df_classificacao.copy()\n",
    "test_df = test_df.rename(columns={'Mensagem': 'comment_text'})\n",
    "test_df = test_df[test_df['comment_text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Pg5LO3A-csZz"
   },
   "outputs": [],
   "source": [
    "#test_df[\"comment_text\"] = test_df[\"comment_text\"].apply(lambda x: x.lower())\n",
    "test_df[\"comment_text\"] = test_df[\"comment_text\"].apply(lambda x: removeruidos(x))\n",
    "test_df[\"comment_text\"] = test_df[\"comment_text\"].apply(lambda x: deEmojify(x))\n",
    "#test_df[\"comment_text\"] = test_df[\"comment_text\"].swifter.allow_dask_on_strings(enable=True).apply(lambda x: corretorGramatical(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6Hb8rKRpnoVE"
   },
   "outputs": [],
   "source": [
    "#test_df = test_df.drop('Positiva', 1)\n",
    "#test_df = test_df.drop('Neutra', 1)\n",
    "#test_df = test_df.drop('Negativa', 1)\n",
    "#test_df = test_df.drop('Unnamed: 0', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GYEE6DKQTQTK"
   },
   "outputs": [],
   "source": [
    "test_df = test_df.join(one_hot)\n",
    "test_cols = test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "peG-VFK1TbiS",
    "outputId": "fe12193f-dae0-48b2-92e0-1e4ed9ee636f"
   },
   "outputs": [],
   "source": [
    "test_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wMM6CHCsd32i"
   },
   "outputs": [],
   "source": [
    "test_label_cols = list(test_cols[6:])\n",
    "test_df['one_hot_labels'] = list(test_df[test_label_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VZRFcRiE7S7W",
    "outputId": "da6319dd-d20e-4fb0-ddb9-3ab1b050d5ac"
   },
   "outputs": [],
   "source": [
    "test_label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "clw0953feQsF",
    "outputId": "fe5d4faa-f167-4c28-8c40-5d8f4c8ce670"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bRukHZ6sgL7K"
   },
   "outputs": [],
   "source": [
    "# Gathering input data\n",
    "test_labels = list(test_df.one_hot_labels.values)\n",
    "test_comments = list(test_df.comment_text.values)\n",
    "\n",
    "# Encoding input data\n",
    "test_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_token_type_ids = test_encodings['token_type_ids']\n",
    "test_attention_masks = test_encodings['attention_mask']\n",
    "\n",
    "\n",
    "# Make tensors out of data\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_token_types = torch.tensor(test_token_type_ids)\n",
    "\n",
    "# Create test dataloader\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQsS3FF3gaSI"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "#track variables\n",
    "logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "  with torch.no_grad():\n",
    "    # Forward pass\n",
    "    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    b_logit_pred = outs[0]\n",
    "    pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "    pred_label = pred_label.to('cpu').numpy()\n",
    "    b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "  tokenized_texts.append(b_input_ids)\n",
    "  logit_preds.append(b_logit_pred)\n",
    "  true_labels.append(b_labels)\n",
    "  pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "# Converting flattened binary values to boolean values\n",
    "true_bools = [tl==1 for tl in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mh6UxrugcNN"
   },
   "outputs": [],
   "source": [
    "pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
    "\n",
    "# Print and save classification report\n",
    "#print('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "#print('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\n",
    "clf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\n",
    "pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "#print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtNu1Ccwgfdh"
   },
   "outputs": [],
   "source": [
    "idx2label = dict(zip(range(14),label_cols))\n",
    "print(idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8DcQHhCghrF"
   },
   "outputs": [],
   "source": [
    "# Getting indices of where boolean one hot vector true_bools is True so we can use idx2label to gather label names\n",
    "true_label_idxs, pred_label_idxs=[],[]\n",
    "for vals in true_bools:\n",
    "  true_label_idxs.append(np.where(vals)[0].flatten().tolist())\n",
    "for vals in pred_bools:\n",
    "  pred_label_idxs.append(np.where(vals)[0].flatten().tolist())\n",
    "\n",
    "# Gathering vectors of label names using idx2label\n",
    "true_label_texts, pred_label_texts = [], []\n",
    "for vals in true_label_idxs:\n",
    "  if vals:\n",
    "    true_label_texts.append([idx2label[val] for val in vals])\n",
    "  else:\n",
    "    true_label_texts.append(vals)\n",
    "\n",
    "for vals in pred_label_idxs:\n",
    "  if vals:\n",
    "    pred_label_texts.append([idx2label[val] for val in vals])\n",
    "  else:\n",
    "    pred_label_texts.append(vals)\n",
    "\n",
    "# Decoding input ids to comment text\n",
    "comment_texts = [tokenizer.decode(text,skip_special_tokens=True,clean_up_tokenization_spaces=False) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAIt-S2MgnKf"
   },
   "outputs": [],
   "source": [
    "# Converting lists to df\n",
    "comparisons_df = pd.DataFrame({'comment_text': comment_texts, 'pred_labels':pred_label_texts})\n",
    "comparisons_df['pred_labels'] = [','.join(map(str, l)) for l in comparisons_df['pred_labels']]\n",
    "comparisons_df.to_csv('comparisons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k4UwtSnhN6Q"
   },
   "outputs": [],
   "source": [
    "comparisons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1M7v2VIhoHd"
   },
   "outputs": [],
   "source": [
    "df_classificacao = df_classificacao.reindex(columns=['Usuario', 'Mensagem', 'Localizacao', 'candidato'])\n",
    "df_final = df_classificacao.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSYxTQSUkJsf"
   },
   "outputs": [],
   "source": [
    "df_final['Polaridade'] = comparisons_df['pred_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCaVXu1MPPZ4"
   },
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AgQkIhjpEpA"
   },
   "outputs": [],
   "source": [
    "df_final.to_csv('/content/drive/MyDrive/TCC/model_bert_tcc/df_final.csv')\n",
    "files.download('df_final.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TCC_Leonardo_Capellaro_Eleicoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
